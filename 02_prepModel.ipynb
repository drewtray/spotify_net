{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Update spotify net by assigning tracks with above a playcount threshold\n",
    "  to an existing public playlist and removing from net\n",
    "output-file: prepmodel.html\n",
    "title: Update spotify net/assign tracks\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp prepModel_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import requests\n",
    "import boto3\n",
    "import json\n",
    "from io import BytesIO\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_s3():\n",
    "\n",
    "    s3_resource = boto3.resource('s3')\n",
    "\n",
    "    scaler = s3_resource.Object('spotify-net', 'scaler').get()\n",
    "    scaler = pickle.loads(scaler['Body'].read())\n",
    "\n",
    "    svd = s3_resource.Object('spotify-net', 'svd').get()\n",
    "    svd = pickle.loads(svd['Body'].read())\n",
    "\n",
    "    spot_tracks = pd.read_csv('s3://spotify-net/newer_tracks.csv', index_col=0)\n",
    "    last_tracks = pd.read_csv('s3://spotify-net/df_tracks.csv', index_col=0)\n",
    "\n",
    "    gen_series = pd.read_csv('s3://spotify-net/genres_svd.csv', index_col=0, squeeze=True)\n",
    "    key_series = pd.read_csv('s3://spotify-net/key_list.csv', index_col=0, squeeze=True)\n",
    "    time_series = pd.read_csv('s3://spotify-net/timeSig_list.csv', index_col=0, squeeze=True)\n",
    "\n",
    "    s3_dict = {\n",
    "        'scaler': scaler,\n",
    "        'svd': svd,\n",
    "        'spot_tracks': spot_tracks,\n",
    "        'last_tracks': last_tracks,\n",
    "        'gen_series': gen_series,\n",
    "        'key_series': key_series,\n",
    "        'time_series': time_series\n",
    "    }\n",
    "\n",
    "    return s3_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def merge_frame(spot_tracks, last_tracks):\n",
    "    spot_tracks[['name', 'artist']] = spot_tracks[['name', 'artist']].applymap(lambda x: x.upper())\n",
    "    last_tracks[['name', 'artist']] = last_tracks[['name', 'artist']].applymap(lambda x: x.upper())\n",
    "\n",
    "    df_classify = pd.merge(last_tracks, spot_tracks, on=['name', 'artist'])\n",
    "    to_drop = ['playcount', 'added at', 'artist id', 'id', 'playlist id', 'type', 'track_href', 'analysis_url', 'type', 'diff']\n",
    "    df_classify = df_classify.drop(to_drop, axis=1)\n",
    "\n",
    "    return df_classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dummies_and_scale(df_classify, constant, scaler):\n",
    "   # log-transform\n",
    "   # c = 0.0000001\n",
    "    c=constant\n",
    "    df_classify[['speechiness', 'acousticness', 'instrumentalness']] = df_classify[['speechiness', 'acousticness', 'instrumentalness']] + c\n",
    "    df_classify[['speechiness', 'acousticness', 'instrumentalness']] = np.log(df_classify[['speechiness', 'acousticness', 'instrumentalness']])\n",
    "\n",
    "   # one-hot\n",
    "    df_track = pd.get_dummies(df_classify , prefix=['key', 'time_signature'], columns=['key', 'time_signature'])\n",
    "\n",
    "   # standardScaler\n",
    "    scale_col = ['danceability', 'energy', 'loudness',\n",
    "       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n",
    "       'valence', 'tempo', 'duration_ms']\n",
    "    df_track[scale_col].head()\n",
    "    df_track[scale_col] = scaler.transform(df_track[scale_col])\n",
    "\n",
    "    return df_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def full_frame(df_track, gen_series, svd, key_series, time_series):\n",
    "\n",
    "    curr_gen = df_track.loc[:, df_track.columns.str.startswith('genre_')]\n",
    "    full_gen = pd.DataFrame(np.zeros((len(df_track), len(gen_series.tolist()))) , columns=gen_series.tolist())\n",
    "    full_gen = full_gen.add_prefix('genre_')\n",
    "    full_gen.update(curr_gen)\n",
    "    full_gen.columns = full_gen.columns.str.replace('genre_', '')\n",
    "\n",
    "    test_trans = svd.transform(full_gen)\n",
    "    test_trans = pd.DataFrame(test_trans)\n",
    "    test_trans = test_trans.add_prefix('genre_')\n",
    "\n",
    "    df_track = df_track.loc[:, ~df_track.columns.str.startswith('genre_')]\n",
    "    df_track = pd.concat([df_track, test_trans], axis=1)\n",
    "\n",
    "    curr_key = df_track.loc[:, df_track.columns.str.startswith('key_')]\n",
    "    full_key = pd.DataFrame(np.zeros((len(df_track), len(key_series.tolist()))) , columns=key_series.tolist())\n",
    "    full_key.update(curr_key)\n",
    "\n",
    "    df_track = df_track.loc[:, ~df_track.columns.str.startswith('key_')]\n",
    "    df_track = pd.concat([df_track, full_key], axis=1)\n",
    "\n",
    "    curr_time = df_track.loc[:, df_track.columns.str.startswith('time_signature_')]\n",
    "    full_time = pd.DataFrame(np.zeros((len(df_track), len(time_series.tolist()))) , columns=time_series.tolist())\n",
    "    full_time.update(curr_time)\n",
    "\n",
    "    df_track = df_track.loc[:, ~df_track.columns.str.startswith('time_signature_')]\n",
    "    df_track = pd.concat([df_track, full_time], axis=1)\n",
    "    df_track.to_csv('s3://spotify-net/for_prediction.csv')\n",
    "    print(df_track.shape)\n",
    "\n",
    "    print('Uploaded to S3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 90)\n",
      "Uploaded to S3\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "if __name__ == '__main__':\n",
    "    s3_objects = load_s3()\n",
    "    merged_df = merge_frame(s3_objects['spot_tracks'], s3_objects['last_tracks'])\n",
    "    transformed = dummies_and_scale(merged_df, 0.0000001, s3_objects['scaler'])\n",
    "    full_frame(transformed, s3_objects['gen_series'], s3_objects['svd'], s3_objects['key_series'], s3_objects['time_series'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
