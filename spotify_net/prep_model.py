# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_prepModel.ipynb.

# %% auto 0
__all__ = ['ModelPrep']

# %% ../nbs/02_prepModel.ipynb 2
import pandas as pd
import requests
import boto3
import json
from io import BytesIO
import joblib
import pickle
import numpy as np
pd.set_option('display.max_columns', None)

# %% ../nbs/02_prepModel.ipynb 4
class ModelPrep:
    def __init__(self):
        self.scaler = None
        self.svd = None
        self.prepped_frame = None
        self.genre_series = None
        self.key_series = None
        
        
    def load_s3(self):
        '''
        Ok, so it looks like the order in which I'm doing things is:
        1. Load a pretrained scaler
        2. Load a pretrained SVD...model?
        3. Load Spotify data
        4. Load LastFM data
        5. Load a pandas series of genres?
        6. Load a pandas series of eleven keys
        7. Load a pandas series of four time signatures

        Could I instead load a sci-kit learn pipeline?
        '''

        s3_resource = boto3.resource('s3')

        scaler = s3_resource.Object('spotify-net', 'scaler').get()
        scaler = pickle.loads(scaler['Body'].read())
        self.scaler = scaler

        svd = s3_resource.Object('spotify-net', 'svd').get()
        svd = pickle.loads(svd['Body'].read())
        self.svd = svd

        recent_spotify_tracks = pd.read_csv('s3://spotify-net/newer_tracks.csv', index_col=0)
        recent_lastFM_tracks = pd.read_csv('s3://spotify-net/last_fm_recent_tracks.csv', index_col=0)
        recent_spotify_tracks[['name', 'artist']] = recent_spotify_tracks[['name', 'artist']].applymap(lambda x: x.upper())
        recent_lastFM_tracks[['name', 'artist']] = recent_lastFM_tracks[['name', 'artist']].applymap(lambda x: x.upper())
        self.prepped_frame = pd.merge(recent_spotify_tracks, recent_lastFM_tracks, on=['name', 'artist'])
        self.prepped_frame = self.prepped_frame.drop([
            'playcount', 
            'added at', 
            'artist id',
            'id',
            'type',
            'track_href',
            'analysis_url',
            'type',
            'diff',
            'uri'
        ], axis=1) 

        # how is this list of genres being generated?
        genre_series = pd.read_csv('s3://spotify-net/genres_svd.csv', index_col=0, squeeze=True)
        self.genre_series = genre_series

        # These don't need to be loaded from S3, but I'm doing it anyway because...?
        key_series = pd.read_csv('s3://spotify-net/key_list.csv', index_col=0, squeeze=True)
        self.key_series = key_series
        time_signature_series = pd.read_csv('s3://spotify-net/timeSig_list.csv', index_col=0, squeeze=True)
        self.time_signature_series = time_signature_series     
    
    # TODO: should prepped frame be an argument? Why not an instance variable?
    def transform_features(self, constant):
        # ok, so order I'm log transforming before I'm applying the standard scaler later on? What is the interaction between these two operations generally?
        # also, review feature selection process as it relates to feature transformation, and traditional order here

        # log-transform 'speechiness', 'acousticness', 'instrumentalness':
        c=constant
        self.prepped_frame[['speechiness', 'acousticness', 'instrumentalness']] = self.prepped_frame[['speechiness', 'acousticness', 'instrumentalness']] + c
        self.prepped_frame[['speechiness', 'acousticness', 'instrumentalness']] = np.log(self.prepped_frame[['speechiness', 'acousticness', 'instrumentalness']])

        # one-hot 'key', 'time_signature':
        self.prepped_frame = pd.get_dummies(self.prepped_frame , prefix=['key', 'time_signature'], columns=['key', 'time_signature'])

        # apply standardScaler to 'danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms':
        columns_to_scale = ['danceability', 'energy', 'loudness',
        'speechiness', 'acousticness', 'instrumentalness', 'liveness',
        'valence', 'tempo', 'duration_ms']
        self.prepped_frame[columns_to_scale] = self.scaler.transform(self.prepped_frame[columns_to_scale])
    
    def final_prep(self):
        current_genres = self.prepped_frame.loc[:, self.prepped_frame.columns.str.startswith('genre_')]

        # populate with all SVD-trained genres prior to SVD transformation
        all_genres = pd.DataFrame(np.zeros((len(self.prepped_frame), len(self.genre_series))) , columns=self.genre_series.tolist())
        all_genres = all_genres.add_prefix('genre_')
        all_genres.update(current_genres)
        all_genres.columns = all_genres.columns.str.replace('genre_', '')

        # Reduce features to top 60 components
        transformed_genres = pd.DataFrame(self.svd.transform(all_genres))
        transformed_genres = transformed_genres.add_prefix('genre_')

        self.prepped_frame = self.prepped_frame.loc[:, ~self.prepped_frame.columns.str.startswith('genre_')]
        self.prepped_frame = pd.concat([self.prepped_frame, transformed_genres], axis=1)

        current_keys = self.prepped_frame.loc[:, self.prepped_frame.columns.str.startswith('key_')]
        all_keys = pd.DataFrame(np.zeros((len(self.prepped_frame), len(self.key_series))) , columns=self.key_series.tolist())
        all_keys.update(current_keys)

        self.prepped_frame = self.prepped_frame.loc[:, ~self.prepped_frame.columns.str.startswith('key_')]
        self.prepped_frame = pd.concat([self.prepped_frame, all_keys], axis=1)

        current_time_signatures = self.prepped_frame.loc[:, self.prepped_frame.columns.str.startswith('time_signature_')]
        all_time_signatures = pd.DataFrame(np.zeros((len(self.prepped_frame), len(self.time_signature_series))) , columns=self.time_signature_series.tolist())
        all_time_signatures.update(current_time_signatures)

        self.prepped_frame = self.prepped_frame.loc[:, ~self.prepped_frame.columns.str.startswith('time_signature_')]
        self.prepped_frame = pd.concat([self.prepped_frame, all_time_signatures], axis=1)

        # Should probably be a separate method to load
        self.prepped_frame.to_csv('s3://spotify-net/for_prediction.csv')
        print(self.prepped_frame.shape)
        print('Uploaded to S3')

# %% ../nbs/02_prepModel.ipynb 5
if __name__ == '__main__':
    frame = ModelPrep()
    frame.load_s3()
    frame.transform_features(0.0000001)
    frame.final_prep()
